{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "sys.path.append('../benchmarks')\n",
    "from standard_tester import *\n",
    "\n",
    "from runlmc.models.lmc import LMC, _LOG\n",
    "_LOG.setLevel(logging.INFO)\n",
    "logging.getLogger().handlers[0].stream = sys.stdout\n",
    "from runlmc.kern.rbf import RBF\n",
    "from runlmc.models.optimization import AdaDelta\n",
    "from runlmc.models.gpy_lmc import GPyLMC\n",
    "\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# as chosen in Nguyen 2014\n",
    "ks = [RBF(name='rbf0')]\n",
    "ranks = [2]\n",
    "# the columns with nonzero test holdout are in test_fx\n",
    "xss, yss, test_xss, test_yss, test_fx, cols = foreign_exchange_2007()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:runlmc.models.lmc:LMC lmc generating inducing grid n = 3054\n",
      "INFO:runlmc.models.lmc:LMC lmc grid (n = 3054, m = 238) complete, \n",
      "INFO:runlmc.models.lmc:LMC lmc fully initialized\n",
      "starting adadelta {'min_grad_ratio': 0.5, 'step_rate': 1, 'roll': 1, 'max_it': 100, 'decay': 0.9, 'permitted_drops': 5, 'momentum': 0.5, 'verbosity': 20, 'offset': 0.0001}\n",
      "iteration        5 grad norm 2.2796e+02\n",
      "iteration       10 grad norm 2.6322e+02\n",
      "iteration       15 grad norm 1.5836e+02\n",
      "iteration       20 grad norm 2.7646e+02\n",
      "iteration       25 grad norm 3.0805e+02\n",
      "finished adadelta optimization\n",
      "            28 iterations\n",
      "    1.0152e+02 final grad norm\n",
      "    1.0152e+02 final MA(1) grad norm\n",
      "    4.0098e+02 max MA(1) grad norm\n",
      "    norm used inf\n",
      "INFO:runlmc.models.lmc:Using 4 processors in parallel for 150 on-the-fly variance predictions\n",
      "time 30.194994707 smse 0.162128052074 nlpd -4.15247175085\n"
     ]
    }
   ],
   "source": [
    "llgp_time, llgp_smse, llgp_nlpd, lmc = runlmc(\n",
    "    1, None, xss, yss, test_xss, test_yss,\n",
    "    ks, ranks, {'verbosity': 20})\n",
    "print('time', llgp_time, 'smse', llgp_smse, 'nlpd', llgp_nlpd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Interpolating points != inducing points, but might as well give\n",
    "# both good settings for a pretty picture.\n",
    "m = len(lmc.inducing_grid)\n",
    "cogp_time, cogp_smse, cogp_nlpd, cogp_mu, cogp_var = cogp_fx2007(1, m)\n",
    "print('m', m, 'time', cogp_time, 'smse', cogp_smse, 'nlpd', cogp_nlpd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    test_fx = ['CAD', 'JPY', 'AUD']\n",
    "    cogp_mu = pd.read_csv('/tmp/cogp-fx2007-mu', header=None, names=test_fx)\n",
    "    cogp_var = pd.read_csv('/tmp/cogp-fx2007-var', header=None, names=test_fx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_xs = np.arange(min(xs.min() for xs in xss), max(xs.max() for xs in xss) + 1)\n",
    "test_ix = {col: list(cols).index(col) for col in test_fx}\n",
    "pred_xss = [all_xs if col in test_fx else np.array([]) for col in cols]\n",
    "lmc.prediction = 'exact'\n",
    "pred_yss, pred_vss = lmc.predict(pred_xss)\n",
    "pred_yss = {col: ys for col, ys in zip(cols, pred_yss)}\n",
    "pred_vss = {col: vs for col, vs in zip(cols, pred_vss)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_, axs = plt.subplots(ncols=3, figsize=(16, 4))\n",
    "for col, ax in zip(test_fx, axs):\n",
    "    \n",
    "    # Prediction on entire domain for COGP\n",
    "    ax.plot(all_xs, cogp_mu[col], c='black', ls='-')\n",
    "    sd = np.sqrt(cogp_var[col])\n",
    "    top = cogp_mu[col] + 2 * sd\n",
    "    bot = cogp_mu[col] - 2 * sd\n",
    "    ax.fill_between(all_xs, bot, top, facecolor='grey', alpha=0.2)\n",
    "    \n",
    "    # Prediction for LLGP\n",
    "    ax.plot(all_xs, pred_yss[col], c='red')\n",
    "    sd = np.sqrt(pred_vss[col])\n",
    "    top = pred_yss[col] + 2 * sd\n",
    "    bot = pred_yss[col] - 2 * sd\n",
    "    ax.fill_between(all_xs, bot, top, facecolor='green', alpha=0.3)    \n",
    "    \n",
    "    # Actual holdout\n",
    "    marker_size = 5\n",
    "    test_xs = test_xss[test_ix[col]]\n",
    "    test_ys = test_yss[test_ix[col]]\n",
    "    ax.scatter(test_xs, test_ys, c='blue', edgecolors='none', s=marker_size, zorder=11)\n",
    "    \n",
    "    # Rest of image (training)\n",
    "    rest_xs = xss[test_ix[col]]\n",
    "    rest_ys = yss[test_ix[col]]\n",
    "    ax.scatter(rest_xs, rest_ys, c='magenta', edgecolors='none', s=marker_size, zorder=10)\n",
    "        \n",
    "    ax.set_xlim([0, 250])\n",
    "    ax.set_title('output {} (95%)'.format(col))\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sum(map(len, xss)), sum(map(len, test_xss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# warning: precompute takes a while, ~10 min on a 4-core 2015 laptop\n",
    "methods = ['on-the-fly', 'exact', 'precompute']\n",
    "results = []\n",
    "for m in methods:\n",
    "    lmc.prediction = m\n",
    "    with contexttimer.Timer() as t:\n",
    "        pred_yss, pred_vss = lmc.predict(test_xss)\n",
    "    print('method {: <12} smse {:6.4f} nlpd {:13.4e} time {}'.format(\n",
    "            m, \n",
    "            smse(test_yss, pred_yss, yss),\n",
    "            nlpd(test_yss, pred_yss, pred_vss),\n",
    "            t.elapsed))\n",
    "    results.append(np.hstack(pred_vss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "onthefly, exact, precomp = results\n",
    "def rel_norm_diff(exact, x):\n",
    "    diff = np.linalg.norm(exact - x)\n",
    "    orig = np.linalg.norm(exact)\n",
    "    return diff / orig\n",
    "print('on the fly ', rel_norm_diff(exact, onthefly))\n",
    "print('precomputed', rel_norm_diff(exact, precomp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = len(lmc.y)\n",
    "m = len(lmc.inducing_grid)\n",
    "D = len(lmc.noise)\n",
    "Q = len(lmc.kernels)\n",
    "R = int(sum(ranks) / Q)\n",
    "paramstr = 'n={},m={},D={},Q={},R={}'.format(n, m, D, Q, R)\n",
    "paramstr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# warning: last one takes ~10 min on a 4-core 2015 laptop\n",
    "samples = [100, 500, 1000]\n",
    "samp_results = []\n",
    "for s in samples:\n",
    "    lmc.prediction = 'sample'\n",
    "    lmc.variance_samples = s\n",
    "    if 'sampled_nu' in lmc._cache:\n",
    "        del lmc._cache['sampled_nu']\n",
    "    with contexttimer.Timer() as t:\n",
    "        pred_yss, pred_vss = lmc.predict(test_xss)\n",
    "    print('method {: <12} smse {:6.4f} nlpd {:13.4e} time {}'.format(\n",
    "            'sample {:5d}'.format(s), \n",
    "            smse(test_yss, pred_yss, yss),\n",
    "            nlpd(test_yss, pred_yss, pred_vss),\n",
    "            t.elapsed))\n",
    "    samp_results.append(np.hstack(pred_vss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.semilogy(exact, label='exact')\n",
    "lss = [':', '--', '-']\n",
    "for ns, res, ls in zip(samples, samp_results, lss):\n",
    "    plt.semilogy(res, label=r'$N_s={}$'.format(ns), ls=ls)\n",
    "plt.title(r'pred $\\sigma^2$ for ${}$'.format(paramstr))\n",
    "plt.xlabel('test point index')\n",
    "plt.ylabel('predictive variance')\n",
    "plt.legend(bbox_to_anchor=(1.05, 0.6), loc=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# also takes ~10 min\n",
    "lmc_with_metrics = LMC(xss, yss, kernels=ks, ranks=ranks, metrics=True)\n",
    "lmc_with_metrics.optimize(optimizer=AdaDelta(\n",
    "        # Force full 40 iterations with ratio = 0\n",
    "        verbosity=10, max_it=40, min_grad_ratio=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def moving_average(a, n) :\n",
    "    sums = np.add.accumulate(a, dtype=float)\n",
    "    sums[n:] = sums[n:] - sums[:-n]\n",
    "    sums[n:] /= n\n",
    "    sums[:n] /= np.arange(1, n + 1)\n",
    "    return sums\n",
    "\n",
    "def div_rolled_max(a, n):\n",
    "    ma = moving_average(a, n)\n",
    "    roll_max = np.maximum.accumulate(ma)\n",
    "    return ma / roll_max * roll_max[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(lmc_with_metrics.metrics.iterations, label='minres iterations')\n",
    "n = len(lmc_with_metrics.y)\n",
    "plt.axhline(n, label='iterartion cutoff (n)', c='r')\n",
    "plt.legend(bbox_to_anchor=(1, 0.6), loc=2)\n",
    "plt.show()\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.plot(lmc_with_metrics.metrics.log_likely, c='r')\n",
    "ax1.set_ylabel('log likelihood', color='r')\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(lmc_with_metrics.metrics.grad_norms, c='b', label='raw norms')\n",
    "\n",
    "# Roll = 1 by default\n",
    "ax2.plot(div_rolled_max(lmc_with_metrics.metrics.grad_norms, 1), c='g', ls='--', \n",
    "         label='div roll max')\n",
    "ax2.set_ylabel('grad norms', color='b')\n",
    "fig.tight_layout()\n",
    "plt.legend(bbox_to_anchor=(1.2, 0.6), loc=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
