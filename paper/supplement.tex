\documentclass{article}

\PassOptionsToPackage{numbers, compress}{natbib}
% Camera ready: \usepackage[final]{nips_2017}
\usepackage{nips_2017}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{graphicx}
\usepackage{algorithm}% http://ctan.org/pkg/algorithms
\usepackage{algpseudocode}% http://ctan.org/pkg/algorithmicx
\usepackage{booktabs}
\usepackage{subfigure}

\input{defs}

\title{Supplementary Material for Large Linear Multi-output Gaussian Process Learning for Time Series}

\author{
  Vladimir Feinberg, Li-Fang Cheng, {Barbara Engelhardt}, {Kai Li}\\
  Department of Computer Science\\
  Princeton University\\
  Princeton, NJ 08544 \\
  \texttt{\{vyf,lifangc,bee,li\}@princeton.edu} \\
}

\begin{document}

\maketitle

\section{Implementation Details}

LLGP was implemented in Python 3 from the Anaconda, which offered an Intel MKL-linked scipy \cite{scipy}. The code made heavy use of other packages, namely climin \cite{climin}, GPy \cite{gpy}, and paramz \cite{paramz}. Code and benchmarks are available at \texttt{<anonymous repository>}.

Application of our approach to all replication studies was carried out on a large server in a multiprogramming environment: CentOS 6.5 with 80 Intel(R) Xeon(R) CPU E7-4870 @ 2.40GHz. The representation evaluation benchmarks were done at once on a cluster of machines running CentOS 5.2-5.9 with Intel(R) Xeon(R) CPU E5430 @ 2.66GHz, where these jobs ran on a single thread per CPU.

\bibliographystyle{unsrtnat}
\bibliography{paper}

\end{document}

