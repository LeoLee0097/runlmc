publishing results into out directory /data/vladf/runlmc/benchmarks/pictures/out/
FX2007 picture
training LLGP
starting adadelta {'step_rate': 1, 'decay': 0.9, 'momentum': 0.5, 'offset': 0.0001, 'max_it': 100, 'verbosity': 20, 'min_grad_ratio': 0.2, 'permitted_drops': 5, 'callback': <function AdaDelta.noop at 0x7f3188d9d400>}
iteration        5 grad norm 5.6933e+02
iteration       10 grad norm 6.4689e+02
iteration       15 grad norm 2.7563e+02
iteration       20 grad norm 1.5316e+02
iteration       25 grad norm 1.1084e+02
finished adadelta optimization
            27 iterations
    7.3701e+01 final grad norm
    norm used inf
    time 52.53013987373561 smse 0.229681760204 nlpd -3.47453494589
training COGP

time   100.1126 (    0.0000) smse     0.2771 (    0.0000) nlpd    13.0297 (    0.0000)
fx2007graph.pdf
weather picture
training LLGP
starting adadelta {'step_rate': 1, 'decay': 0.9, 'momentum': 0.5, 'offset': 0.0001, 'max_it': 100, 'verbosity': 20, 'min_grad_ratio': 0.1, 'permitted_drops': 5, 'callback': <function AdaDelta.noop at 0x7f3188d9d400>}
iteration        5 grad norm 5.5874e+03
iteration       10 grad norm 3.0929e+03
iteration       15 grad norm 1.2779e+03
iteration       20 grad norm 1.2048e+03
iteration       25 grad norm 1.1875e+03
iteration       30 grad norm 1.0242e+03
iteration       35 grad norm 8.0519e+02
iteration       40 grad norm 6.5273e+02
finished adadelta optimization
            40 iterations
    6.5273e+02 final grad norm
    norm used inf
    time 232.98593415599316 smse 0.143754812376 nlpd 3.48898658493
training COGP

time   461.1744 (    0.0000) smse     0.0811 (    0.0000) nlpd    21.8670 (    0.0000)
weather.pdf
running MINRES metrics
starting adadelta {'step_rate': 1, 'decay': 0.9, 'momentum': 0.5, 'offset': 0.0001, 'max_it': 35, 'verbosity': 10, 'min_grad_ratio': 0, 'permitted_drops': 5, 'callback': <function AdaDelta.noop at 0x7f267aed5268>}
iteration        3 grad norm 5.5819e+02
iteration        6 grad norm 5.8407e+02
iteration        9 grad norm 6.4094e+02
iteration       12 grad norm 6.0113e+02
iteration       15 grad norm 2.7563e+02
iteration       18 grad norm 4.5504e+02
iteration       21 grad norm 4.0505e+01
iteration       24 grad norm 1.2225e+02
iteration       27 grad norm 7.3701e+01
iteration       30 grad norm 3.5786e+02
iteration       33 grad norm 1.8519e+02
finished adadelta optimization
            35 iterations
    2.6334e+02 final grad norm
    norm used inf
iterations.pdf
running_cutoff.pdf
